\documentclass[12pt]{report}      
\usepackage{} 
\usepackage{amsthm}
\usepackage{mathrsfs}
\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\newtheorem{theorem}{Theorem}[chapter]
\newtheorem{lemma}{Lemma}[section]
\newtheorem{corollary}[lemma]{Corollary}
\newtheorem{example}[lemma]{Example}
\newtheorem{definition}{Definition}[chapter]
\newtheorem{proposition}{Proposition}[chapter]
\newtheorem{remark}[lemma]{Remarque}
\newtheorem{assumption}{Assumption}[chapter]
\usepackage{boxedminipage2e}
\usepackage[framemethod=TikZ]{mdframed}
\newcounter{theo}[section]
\usepackage{xcolor}
 
\renewcommand{\thetheo}{\arabic{section}.\arabic{theo}}
\newenvironment{theo}[2][]{%
    \refstepcounter{theo}
\newcommand*{\defeq}{\mathrel{\vcenter{\baselineskip0.5ex \lineskiplimit0pt
                     \hbox{\scriptsize.}\hbox{\scriptsize.}}}%
                     =} 
    % Code for box design goes here.
 
\begin{mdframed}[]\relax}{%
\end{mdframed}}
\ifstrempty{#1}%
% if condition (without title)
{\mdfsetup{%
    frametitle={%
        \tikz[baseline=(current bounding box.east),outer sep=0pt]
        \node[anchor=east,rectangle,fill=black!20]
        {\strut Theorem~\thetheo};}
    }%
% else condition (with title)
}{\mdfsetup{%
    frametitle={%
        \tikz[baseline=(current bounding box.east),outer sep=0pt]
        \node[anchor=east,rectangle,fill=black!20]
        {\strut Theorem~\thetheo:~#1};}%
    }%
}%
% Both conditions
\mdfsetup{%
    innertopmargin=10pt,linecolor=black!20,%
    linewidth=4pt,topline=true,%
    frametitleaboveskip=\dimexpr-\ht\strutbox\relax%
}

\usepackage{amsfonts}
\usepackage{mathtools} 
\usepackage{enumitem} 
\usepackage{framed,tikz}
\usepackage[linesnumbered,ruled]{algorithm2e}
\definecolor{fond}{gray}{0.91} 
\newenvironment{cadrecode}{ 
	\def\FrameCommand{{\color[HTML]{888888}\vrule width 			3pt}\colorbox{fond}} 
	\MakeFramed {\advance\hsize-\width \FrameRestore}}%
{\endMakeFramed}
\usepackage[a4paper,top=1.5cm,bottom=1.5cm,left=1.5cm,right=1.5cm,marginparwidth=1.75cm]{geometry}
\newcommand{\E}{\mathrm{E}}

\newcommand{\Var}{\mathrm{Var}}

\newcommand{\Cov}{\mathrm{Cov}}
\renewcommand{\baselinestretch}{1.4}
\usepackage{empheq}
\DeclareMathOperator*\argmin{arg\,min}
\DeclareMathOperator*\argmax{arg\,max}
\DeclareMathOperator\prox{Prox}
% Command "alignedbox{}{}" for a box within an align environment
% Source: http://www.latex-community.org/forum/viewtopic.php?f=46&t=8144
\newlength\dlf  % Define a new measure, dlf
\newcommand\alignedbox[2]{
% Argument #1 = before & if there were no box (lhs)
% Argument #2 = after & if there were no box (rhs)
&  % Alignment sign of the line
{
\settowidth\dlf{$\displaystyle #1$}  
    % The width of \dlf is the width of the lhs, with a displaystyle font
\addtolength\dlf{\fboxsep+\fboxrule}  
    % Add to it the distance to the box, and the width of the line of the box
\hspace{-\dlf}  
    % Move everything dlf units to the left, so that & #1 #2 is aligned under #1 & #2
\boxed{#1 #2}
    % Put a box around lhs and rhs
}
}
 
\begin{document}

\title{SVRG, L-Katyusha and Optimality}

\author{Badr-Eddine ABDALLAOUI}

\maketitle


\newpage



\tableofcontents

\chapter{Introduction and Problem Setting}
In this short report we consider the following optimization problem:


\begin{equation} \label{pb1}
\boxed{\min_{x\in \mathbb{R}^d} f(x)=\frac{1}{n}\sum_{i=1}^nf_i(x)}
\end{equation}

Where :
\begin{itemize}
    \item $n,d \in\mathbb{N}^*$
    \item $f_i :\mathbb{R}^d\longrightarrow\mathbb{R}$ are differentiable, convex and $\beta$-smooth for some $\beta>0$
    \item $F$ is $\alpha$-strongly convex for some $\alpha>0$
\end{itemize}



We will discuss in detail two versions of the SGD : SVRG which is a variance reduced version of the SGD, and L-Katyusha which is a loopless (i.e without the outer loop), accelerated version of SVRG. For each algorithm, we conduct a convergence analysis and assess the oracle complexity. We conclude by discussing the optimality and showing that L-Katyusha is optimal in some sense that we will define. 

\chapter{SVRG}
\section{Introduction to variance reduced techniques}
While the diminishing stepsize SGD is guaranteed to converge in expectation, the convergence rate is of order $\mathcal{O}(\frac{1}{s})$,  where $s$ is the number of iterations, and hence one may wonder if there are some other ways to improve this rate without deteriorating the oracle complexity.\\
\\Variance reduction technique is a novel approach that enhances the convergence rate of the SGD. Recall that for the basic SGD, the variance of the gradient was partially responsible of the failure of the convergence guarantee. In this context, variance reduction aims to get rid of the uniform bound on the variance of the noisy gradient by defining a less severe strategy : at each iteration, instead of imposing a completely new stochastic direction :
$$D\gets \nabla f_i(x)\ \ \ \ i\text{\ sampled\ uniformly}$$
we rather follow a less severe approach by keeping a fixed point $x_s$, setting $\Tilde{x}_0=x_s$ and then calculating the updates $\Tilde{x}_1,...,\Tilde{x}_m$ following the 2-steps iteration:



\begin{enumerate} 
    \item  $D_k\gets \nabla f_i(\Tilde{x}_k)-\nabla f_i(x_s)+\nabla f(x_s)\ \ \ \ \ i\text{\ sampled\ uniformly}$ 
    \hfill\refstepcounter{equation}(\theequation)

    \item $\Tilde{x}_{k+1}\gets \Tilde{x}_k + D_k$
\end{enumerate}




Where $m$ is called the update frequency.

After the $m$ steps, we set a new value for $x_{s+1}$ as a function of $\{\Tilde{x}_1,...,\Tilde{x}_k\}$  and repeat the same $m$ updates, we say then that we have done one epoch. The whole process is repeated $T$ times and hence we get one inner loop and one outer loop.

This new estimate of the gradient $D_k$, besides its unbiasedeness, is likely to have a lower variance that the basic direction $D$ \cite{allen2017katyusha}.

The direction update (2.1) has many variants and has been designed in various forms, forming what can be called as variance reduced SGD methods. In this context, the first pioneering work was done in \cite{schmidt2017minimizing}, where a new algorithm : Stochastic Averaged Gradient (SAG) was studied. This algorithm enhances the convergence rate from sublinear rate $\mathcal{O}(\frac{1}{s})$ to linear rate $\mathcal{O}(\gamma^s)$ for some $\gamma<1$. \\
\\Later on, many versions and improvements were done, including SVRG \cite{johnson2013accelerating}, SAGA \cite{defazio2014saga}, FINITO \cite{defazio2014finito}.
The iteration discussed above formally concerns SVRG.
It is worth mentioning, that while the above methods are described as variance reduced in literature, not all of them have a provable reduced variance in comparison with basic SGD. We can see "variance reduced" as a generic name rather than a systematic method. \\
\\In the following we will be discussing the SVRG in more details. We choose to study SVRG over other variants due do its relatively simple convergence analysis as well as its enhancement of some other previous variance reduced methods, such as SAG and SAGA.


\section{SVRG Algorithm}
The SVRG relies on Iteration (2.1), in which the point $x_s$ is updated every epoch based on the updates $\{\Tilde{x}_1,...,\Tilde{x}_m\}$. The point $x_s$ can be updated using one of the three options : 
\begin{enumerate}
    \item Option I : we choose the last update $x_s=\Tilde{x}_k$
    \item Option II : we randomly sample $t\in\{1,...,m\}$, and set $x_s=\Tilde{x}_t\}$
    \item Option III: we choose an average of the updates : $x_s=\frac{1}{m}\sum\limits_{i=1}^m \Tilde{x}_t$ 
\end{enumerate}
Algorithm \ref{SVRG} describe the pseudo-code of the SVRG with different options. One may be confused whether an option should be fixed or can be changed at each epoch. As this wasn't specified in articles we came across so far, we claim that a mixing of options I and II is possible and this strategy yields the same convergence result as if we fix one of the two. 

\begin{algorithm}[H]\label{SRG}
    \SetKwInOut{Input}{Input}
    \SetKwInOut{Output}{Output}
    \Input{ $\eta>0$, $x_0\in\mathbb{R}^d$, update frequency $m\geq 1$}
    %\Output{$\gcd(a,b)$}
    $x=x_0$\;
    \For{$s=1,2,...,T$}{
            \State 
            $x=x_{s-1}$
            
            $\mu=\frac{1}{n}\sum_{i=1}^n\nabla f_i(x)$
            
            $\Tilde{x}_0=x$
            
            \For{$t=1,2,...,m$}{
            Sample uniformly $i_{t}\in\{1,...,n\}$
            
            $\Tilde{x}_t=\Tilde{x}_{t-1}-\eta(\nabla f_{i_t}(\Tilde{x}_{t-1})-\nabla f_{i_t}({x})+\mu)$
            
            \EndFor}
                
  
            
            
            Option I :  Sample uniformly $t\in\{1,...,m\}$,  $x_s=\Tilde{x}_t$
            
            Option II : $x_s=\frac{1}{m}\sum\limits_{i=1}^m \Tilde{x}_{t-1}$
            
            Option III : $x_s=\Tilde{x}_m$
            
            \EndFor}
    \caption{SVRG}
\end{algorithm}

Now we state the theorem that proves the convergence of the SVRG.

\begin{theo}
LLet us consider applying SVRG to solving Problem \ref{pb1}.
Let $x^*=\argmin_{x\in\mathbb{R}^d} f(x)$.
\begin{itemize}
    \item Under options I or II:
    
    Let $\eta<\frac{1}{4\beta}$, and $m$ be sufficiently large such that the following holds:
\begin{equation}
    \gamma=\frac{1}{\alpha\eta(1-2\beta\eta)m}+\frac{2\eta\beta}{1-2\beta\eta}<1
\end{equation}   
Then,
\begin{equation}
    \E[f(X_s)-f(x^*)]\leq \gamma^s(f(x_0)-f(x^*))
\end{equation}

    \item Under Option III:
    
    Let $m$ and $\eta$ be chosen such that the following holds:
\begin{equation}
    \mu=\kappa\left(\left(1-2\eta\alpha(1-\eta\beta)\right)^m+\frac{2\eta\beta^2}{\alpha(1-\eta\beta)}\right)<1
\end{equation}
Then,
\begin{equation}
    \E[f(X_s)-f(x^*)]\leq \mu^s(f(x_0)-f(x^*))
\end{equation}

\end{itemize}




\end{theo}
\begin{proof}
See next section
\end{proof}
\section{Convergence analysis}
In this section we prove Theorem 2.1. We combine ideas from \cite{johnson2013accelerating}, \cite{bubeck2015convex},  \cite{hu2018dissipativity} and \cite{tan2016barzilai}. 
The very first convergence analysis that is presented in the original SVRG paper \cite{johnson2013accelerating} tackles only Option I although Option II was used in experiments. The convergence of Option II appears in \cite{bubeck2015convex} and option III was recently proved in \cite{tan2016barzilai}. \cite{hu2018dissipativity} also proved Option III with a slightly better improvement in the decaying constant yet it uses an elaborated tool from dynamical system theory and dissipativity theory that will not be discussing here. However we show that the new decaying constant could be obtained by slighlty modifying the proof given in \cite{tan2016barzilai}.


\subsection{Option I \& II}

\\We begin by a useful lemma:
\begin{lemma}\label{lemma1}
We have for any $x\in\mathbb{R}^d$:
$$\frac{1}{n}\sum_{i=1}^n\|\nabla f_i(x)-\nabla f_i(x^*)\|^2\leq 2\beta(f(x)-f(x^*))$$
\end{lemma}

\begin{proof}
Let 
$$g_i(x)=f_i(x)-f_i(x^*)-\nabla f_i(x^*)^\top(x-x^*)$$
By the convexity of $f_i$, we see easily that $g_i$ achieves its minimum at $x^*$.\\
\\Hence, 
\begin{equation}\label{ineq1}
0=g_i(x^*)\leq \min_{\delta\in\mathbb{R}} g_i\left(x-\delta\nabla g_i(x)\right)
\end{equation}
Yet, $g_i\left(x-\eta\nabla g_i(x)\right)=f_i(x-\eta\nabla g_i(x))-f_i(x^*)-\nabla f_i(x^*)^\top(x-\eta\nabla g_i(x)-x^*)$\\

Now using the $\beta$-smoothness of $f_i$, we have:
$$f_i\left(x-\eta\nabla g_i(x)\right)\leq f_i(x) -\eta\nabla f_i(x)^\top\nabla g_i(x)+\frac{\beta}{2}\eta^2\|\nabla g_i(x)\|^2$$
And by using  
$\nabla g_i(x)=\nabla f_i(x)-\nabla f_i(x^*) $, we get

$$g_i\left(x-\eta\nabla g_i(x)\right)\leq g_i(x)-\eta\|\nabla g_i(x) \|^2+\frac{\beta\eta^2}{2}\|\nabla g_i(x)\|^2 $$
Now minimizing the right hand side of the last inequality over $\eta\in\mathbb{R}$, we get:
$$ \min_{\delta\in\mathbb{R}} g_i\left(x-\delta\nabla g_i(x)\right)=g_i(x)-\frac{1}{2\beta}\|\nabla g_i(x)\|^2$$
We conclude by using Inequality \ref{ineq1} and summing over $i$:
$$\frac{1}{n}\sum_{i=1}^n\|\nabla f_i(x)-\nabla f_i(x^*)\|^2\leq 2\beta(f(x)-f(x^*))$$
\end{proof}

\begin{proof}[Proof of Theorem \ref{theo:svrg}]


Let us consider a fixed time step $s$ of the outer loop: $X=X_{s-1}$, $\mu=\nabla f(X)=\frac{1}{n}\sum_{i=1}^n\nabla f_i(X)$
\newline
Consider : $v_t=\nabla f_{I_t}(\Tilde{X}_{t-1})-\nabla f_{I_t}(X)+\mu$
\newline
Now we have the following :
$$\E[\|v_t\|^2|\Tilde{X}_{t-1}]\overset{(a)}{\leq} 2 \E[\|\nabla f_{I_t}(\Tilde{X}_{t-1})-\nabla f_{I_t}(x^*)\|^2|\Tilde{X}_{t-1}]+2\E[\|\nabla f_{I_t}(X)-\nabla f_{I_t}(x^*)-\nabla f(X)\|^2|\Tilde{X}_{t-1}]$$
$$\overset{(b)}{\leq} 2 \E[\|\nabla f_{I_t}(\Tilde{X}_{t-1})-\nabla f_{I_t}(x^*)\|^2|\Tilde{X}_{t-1}]+2\E[\|\nabla f_{I_t}(X)-\nabla f_{I_t}(x^*)\|^2|\Tilde{X}_{t-1}]$$
$$\overset{(c)}{\leq} 4\beta\left( f(\Tilde{X}_{t-1})-2f(x^*)+f(X) \right)$$

Where \begin{itemize}
    \item (a) follows from : for all $x$ and $y$, $(x+y)^2\leq 2(x^2+y^2)$ 
    \item (b) follows from : for all random variables $Z$ and $Y$, $\E[\|Y-E[Y|Z]\|^2|Z]\leq \E\left[\|Y\|^2|Z\right]$
    \item (c) follows from Lemma \ref{lemma1} as $\Tilde{X}_{t-1}$ and $I_t$ are independent.
    
\end{itemize} 
Since $\Tilde{X}_{t}=\Tilde{X}_{t-1}-\eta v_t$, we have
$$\E[\|\Tilde{X}_{t}-x^*\|^2|\Tilde{X}_{t-1}]= \|\Tilde{X}_{t-1}-x^*\|^2-2\eta(\Tilde{X}_{t-1}-x^*)^\top \E[v_t|\Tilde{X}_{t-1}]+\eta^2\E[\|v_t\|^2|\Tilde{X}_{t-1}]$$
$$\E[\|\Tilde{X}_{t}-x^*\|^2|\Tilde{X}_{t-1}]=\E[\|\Tilde{X}_{t-1}-x^*\|^2|\Tilde{X}_{t-1}]-2\eta(\Tilde{X}_{t-1}-x^*)^\top\E[v_t|\Tilde{X}_{t-1}]+\eta^2\E[\|v_t\|^2|\Tilde{X}_{t-1}]$$
$$\leq \|\Tilde{X}_{t-1}-x^*\|^2-2\eta(\Tilde{X}_{t-1}-x^*)^\top\nabla f(\Tilde{X}_{t-1})+4\beta\eta^2\left(f(\Tilde{X}_{t-1})-2f(x^*)+f(X)\right) $$
$$\leq \|\Tilde{X}_{t-1}-x^*\|^2-2\eta\left( f(\Tilde{X}_{t-1})-f(x^*)\right)+4\beta \eta^2 \left( f(\Tilde{X}_{t-1})-2f(x^*)+f(X)\right) $$
$$= \|\Tilde{X}_{t-1}-x^*\|^2-2\eta(1-2\eta\beta)\left(f(\Tilde{X}_{t-1})-f(x^*)\right)+4\beta\eta^2\left(f(X)-f(x^*)\right)$$

Hence, we have :
$$\E[\|\Tilde{X}_{t}-x^*\|^2|\Tilde{X}_{t-1}]+2\eta(1-2\eta\beta)\left(f(\Tilde{X}_{t-1})-f(x^*)\right)\leq \|\Tilde{X}_{t-1}-x^*\|^2 +4\beta\eta^2\left(f(X)-f(x^*)\right)$$

Taking the expectation of both sides and applying the tower property we get:
$$\E[\|\Tilde{X}_{t}-x^*\|^2]+2\eta(1-2\eta\beta)\left(\E[f(\Tilde{X}_{t-1})]-f(x^*)\right)\leq \E[\|\Tilde{X}_{t-1}-x^*\|^2] +4\beta\eta^2\left(\E[f(X)]-f(x^*)\right)$$
After summing over $t$ between $1$ and $m$ we obtain:
$$\E[\|\Tilde{X}_m-x^* \|^2]+2\eta(1-2\eta\beta)\left(\E\left[\sum\limits_{t=1}^mf(\Tilde{X}_{t-1})\right]-mf(x^*)\right) \leq \E[\|\Tilde{X}_0-x^* \|^2]+4\beta m \eta^2(\E[f(X)]-f(x^*))$$
Hence,

\begin{equation}\label{eqsvrg}
2\eta(1-2\eta\beta)\left(\E\left[\sum\limits_{t=1}^mf(\Tilde{X}_{t-1})\right]-mf(x^*)\right) \leq \E[\|\Tilde{X}_0-x^* \|^2]+4\beta m \eta^2(\E[f(X)]-f(x^*))
\end{equation}

note that $\Tilde{X}_0=X$, and by using $\alpha$-strong convexity of $f$, we get:
$$f(x^*)-f(X)\leq \underbrace{\nabla f(x^*)}_{=0}{}^ \top(x^*-X)-\frac{\alpha}{2}\|X-x^*\|^2 $$
Hence,

\begin{equation}
    2\eta(1-2\eta\beta)m\left(\E\left[\frac{1}{m}\sum\limits_{t=1}^mf(\Tilde{X}_{t-1})\right]-f(x^*)\right) \leq (\frac{2}{\alpha}+4\beta m \eta^2)(\E[f(X)]-f(x^*))
\end{equation}



\begin{equation}
   \E\left[\frac{1}{m}\sum\limits_{t=1}^mf(\Tilde{X}_{t-1})\right]-f(x^*) \leq \underbrace{\left(\frac{1}{\alpha\eta(1-2\beta\eta)m}+\frac{2\beta\eta}{1-2\beta\eta}\right)}_{\gamma}(\E[f(X)]-f(x^*))
\end{equation}


Now for each option we have the following :
\begin{itemize}

    \item \underline{Option I} : Sample $I_t$ uniformly in $\{0,...,m-1\}$  and set $X_s= \Tilde{X}_{I_t}$
    $$\E[f(X_s)]=\E\left[\E[f(X_s)|\Tilde{X}_0,...\Tilde{X}_{m-1}]\right]=\E\left[\frac{1}{m}\sum\limits_{t=1}^mf(\Tilde{X}_{t-1})\right]    $$
    \item \underline{Option II} : $X_s= \frac{1}{m}\sum\limits_{t=1}^m\Tilde{X}_{t-1}$
    $$\E[f(X_s)]=\E\left[
    f\left(\frac{1}{m}\sum\limits_{t=1}^m\Tilde{X}_{t-1}\right)\right]$$
    Using the convexity of $f$, we have :
    $$f\left(\frac{1}{m}\sum\limits_{t=1}^m\Tilde{X}_{t-1}\right)\leq \frac{1}{m}\sum\limits_{t=1}^m f(\Tilde{X}_{t-1})$$
     Hence 
    $$\E[f(X_s)]\leq \E\left[\frac{1}{m}\sum\limits_{t=1}^m f(\Tilde{X}_{t-1})\right]$$
    

\end{itemize}

We can conclude that regardless of the chosen option (I or II), we get:
$$   \E[f(X_s)]-f(x^*) \leq \gamma(\E[f(X)]-f(x_0))  $$


Now the above analysis concerns the inner loop at some fixed instance $s$ of the outer loop, and recall that $X=X_{s-1}$

Hence,
$$\E[\left(f({X}_s)-f(x^*)\right) \leq \gamma\E[f({X}_{s-1})-f(x^*)]$$
This last inequality is obtained regardless of the chosen option at epoch $s$. This is why one is free to allow a mix of options choice between epochs.

Finally we get:

$$\E[\left(f({X}_s)-f(x^*)\right)]
\leq \gamma^s \left(f({x}_{0})-f(x^*)\right)$$
\end{proof}


\subsection{Option III}


Let us consider a fixed time step $s$ of the outer loop: $X=X_{s-1}$, $\mu=\nabla f(X)=\frac{1}{n}\sum_{i=1}^n\nabla f_i(X)$
\newline
Consider : $v_t=\nabla f_{I_t}(\Tilde{X}_{t-1})-\nabla f_{I_t}(X)+\mu$

$$\E[\|v_t\|^2|\Tilde{X}_{t-1}]=\E[\|\nabla f_{I_t}(\Tilde{X}_{t-1})-\nabla f_{I_t}(X)+\mu \|^2|\Tilde{X}_{t-1}]$$
$$\leq 2\E[\|\nabla f_{I_t}(\Tilde{X}_{t-1})-\nabla f_{I_t}(x^*) \|^2|\Tilde{X}_{t-1}] + 2\E[\|\nabla f_{I_t}(X)-\nabla f_{I_t}(x^*) + \nabla f(X)\|^2|\Tilde{X}_{t-1}]$$
$$\leq 2\E[\|\nabla f_{I_t}(\Tilde{X}_{t-1})-\nabla f_{I_t}(x^*) \|^2|\Tilde{X}_{t-1}] + 2\E[\|\nabla f_{I_t}(X)-\nabla f_{I_t}(x^*)\|^2|\Tilde{X}_{t-1}]$$
$$\leq 2\beta \E[(\Tilde{X}_{t-1}-x^*)^\top(\nabla f_{I_t}(X)-\nabla f_{I_t}(x^*))|\Tilde{X}_{t-1}]+2\beta^2\|X-x^*\|^2 $$

Now,

$$\E[\|\Tilde{X}_t-x^* \|^2|\Tilde{X}_{t-1},X] = \E[\|\Tilde{X}_{t-1}-\eta v_t-x^* \|^2|\Tilde{X}_{t-1},X]  $$

$$\|{X}_{t-1}-x^* \|^2-2\eta\E[(\Tilde{X}_{t-1})^\top v_t|\Tilde{X}_{t-1},X]+\eta^2\E[\| v_t\|^2|\Tilde{X}_{t-1},X] $$

$$\leq \|{X}_{t-1}-x^* \|^2-2\eta(\Tilde{X}_{t-1})^\top\nabla f(\Tilde{X}_{t-1}+2\eta^2\beta(\Tilde{X}_{t-1}-x^*)^\top\nabla f(\Tilde{X}_{t-1})+2\eta^2\beta^2\|X-x^* \|^2$$

$$=\|{X}_{t-1}-x^*\|^2-2\eta\alpha(1-\eta\beta)\|{X}_{t-1}-x^*\|^2+2\eta^2\beta^2\|{X}-x^*\|^2$$
$$=(1-2\eta\alpha(1-\eta\beta))\|\Tilde{X}_{t-1}-x^* \|^2 +2\eta^2\beta^2\|X-x^* \|^2  $$
Now taking the expectation in both sides, we get
$$\E[ \|\Tilde{X}_t-x^*\|^2]\leq (1-2\eta\alpha(1-\eta\beta))\E[ \|\Tilde{X}_{t-1}-x^*\|^2]+ 2\eta^2\beta^2\E[\|X-x^* \|^2] $$

Applying the inequality recursively for $t=1$ to $t=m$, we get
$$\E[ \|\Tilde{X}_m-x^*\|^2]\leq (1-2\eta\alpha(1-\eta\beta))^m\E[ \|\Tilde{X}_{0}-x^*\|^2]+ 2\eta^2\beta^2\E[\|X-x^* \|^2]\sum\limits_{l=0}^{m-1}(1-2\eta\alpha(1-\eta\beta))^l  $$

Since $\Tilde{X}_0=X_{t-1}$ and by option III  $\Tilde{X}_m=X_{t}$, 
we get
$$\E[ \|X_{t}-x^*\|^2]\leq \left(1-2\eta\alpha(1-\eta\beta))^m+\frac{\eta\beta^2}{\alpha(1-\eta\beta)}\right)\E[ \|\Tilde{X}_{t-1}-x^*\|^2]  $$


By the trick


$$\E[ \|f(X_{t})-f(x^*)\|^2]\leq \underbrace{\kappa\left(1-2\eta\alpha(1-\eta\beta))^m+\frac{\eta\beta^2}{\alpha(1-\eta\beta)}\right)}_{\lambda}\E[ \|f(\Tilde{X}_{t-1})-f(x^*)\|^2]  $$

$$\E[f(X_{s})]-f(x^*)\leq \lambda^s(f(x_{0})-f(x^*))$$


\section{Oracle complexity}
Now we discuss the oracle complexity of SVRG. One important remark here is that since SVRG has hyperparameters : $\eta$ and $m$, one can tune these parameters under their suitable constraints to find the optimal oracle complexity. However, a common way to write the complexity is to find the simplest way to write it as a simple dependency on $n$, $\epsilon$, $\kappa$. This approach makes it clear how the oracle complexity scales with $n$ and $\kappa$ as they are crucial parameters of our Problem \ref{pb1}.
\begin{corollary}


Let us consider our problem setting \ref{pb1}.

We apply SVRG with the following parameters:
$m=\frac{20\beta}{\alpha}$,  $\eta=\frac{1}{10\beta}$

Then, the oracle complexity is:
$$ \mathcal{C}(\mathcal{A},n,\epsilon)\sim \left(n+\kappa \right)\log\left(\frac{1}{\epsilon}\right)$$
Where $\mathcal{A}$ is SVRG with the chosen parameters.
\end{corollary}

\begin{proof}
We have:
$$\gamma=\frac{1}{\alpha\eta(1-2\beta\eta)m}+\frac{2\eta\beta}{1-2\beta\eta}=\frac{2}{\frac{\alpha}{10\beta}(1-1\frac{1}{5})m}+\frac{1}{4}=\frac{7}{8}<1$$


Now if we want : $$\E[\left(f({X}_s)-f(x^*)\right)]<\epsilon $$
It is enough to have : 
$$\gamma^s \left(f({x}_{0})-f(x^*)\right)\approx \epsilon$$
Hence, $$s \approx \frac{\log(\frac{1}{\epsilon})}{\log(\frac{1}{\gamma})} \approx {\log\left(\frac{1}{\epsilon}\right)}$$

Where we dropped the term $\left(f({x}_{0})-f(x^*)\right)$  following the conventions discussed in Section ??.

Recall that $s$ is the number of instances of the outer loop. Let us now come back to the algorithm and assess how many oracle calls do we make at each iteration. First, we make $n$ oracle calls for computing $\mu$, then we have two oracle calls in every iteration of the inner loop. Hence we have in total $n+2m$ oracle calls per outer iteration.
Hence the total number of calls is approximately :
$$(n+40\kappa) {\log\left(\frac{1}{\epsilon}\right)}$$
Now we conclude that :

$$ \mathcal{C}(\mathcal{A},n,\epsilon)\sim \left(n+\kappa \right)\log\left(\frac{1}{\epsilon}\right)$$


\end{proof}

\chapter{L-Katyusha  (Incomplete)}
\section{Introduction}
Let us go back again to the gradient descent method. As discussed in Preliminaries the iteration complexity for Gradient descent when applied to a function $g$ yields the following :
\begin{itemize}
    \item if $g$ is $\beta$-smooth : 
    \begin{equation} \label{eq:smooth}
g(x_t)-g(x^*)\leq \frac{2\beta\| x_0-x^*\|^2}{t-1}
\end{equation}
    \item if $g$ is $\beta$-smooth and $\alpha$-strongly convex :\begin{equation} \label{eq:strg}
g(x_{t+1})-g(x^*)\leq \frac{\beta}{2}\exp(-\frac{4t}{\frac{\beta}{\alpha}+1})\| x_1-x^*\|^2
\end{equation} 
\end{itemize}
Yet, these two convergence rates could be improved by acceleration's methods.
In fact, accelerating gradient descent is a well-known technique that improves the iteration cost and in some cases even reach the optimal convergence rate.
Two famous techniques were proposed : Polyak's Heavy ball and Nesterov's momentum. They both rely on a modification of the basic GD iteration:
\begin{equation}\label{iteration}
    x_{k+1}\gets x_k-\eta_k\nabla g(x_k)
\end{equation}
In fact,
\begin{itemize}
    \item \textbf{Polyak heavy-ball} :
    This method relies on adding a momentum term :
    $$ x_{k+1}\gets x_k-\eta_k\nabla g(x_k)+{\theta_k(x_k-x_{k-1})}$$
    \item \textbf{Nesterov's momentum} : 
    This is more sophisticated than the previous method:
    $$y_{k+1}\gets x_k-\eta_k\nabla g(x_k)$$ $$x_{k+1}\gets y_{k+1}+\mu_k(y_{k+1}-y_k)$$
\end{itemize}

Now, when applying the Polyak's heavy-ball to minimizing .... \textbf{need a good reference for this, momentum better than heavy ball? momentum fails?}
\section{Acceleration in SGD world}
Now, a natural question is whether these acceleration's methods are beneficial to enhance the stochastic Gradient Descent oracle complexity?
In an intuitive way, one should be very careful when blindly accelerating a SGD Algortihm. In fact, as the direction of the search is not guaranteed to be descent at each step, accelerating it in the wrong direction could worsen the convergence rate or even make the SGD fail. \textbf{(construct an example)}\\
\\There have been very recently some attempts for adapting Nesterov's momentum to SGD. \cite{nitanda2016accelerated} suggested an new SGD algorithm that combines the SVRG, mini-batching and Nesterov's acceleration. It showed that the oracle complexity has a slight improvement under some conditions on the batch size. \cite{lin2015universal} suggests a general framework called Catalyst that enables acceleration for many SGD algorithms with provable decrease in oracle complexity.
More importantly, \cite{allen2017katyusha} made a breakthrough by suggesting a modified version of Nesterov's momentum called Katyusha momentum.

This acceleration, when applied to SVRG yields the first known optimal Oracle complexity for Problem \ref{pb1} as we will discuss in the next chapter.
\subsection{Katyusha momentum}

Description of Katyusha momentum+motivation+Katyusha algorithm for Problem \ref{pb1}

\subsection{L-Katyusha : a simplified Katyusha}
In the original Katyusha paper \cite{allen2017katyusha} only the composite case was discussed and the condition number is $\kappa=\frac{\beta}{\alpha}$ such that $\alpha$ is now the strong-convexity constant of $\psi$. If we want to apply the result for our Problem (\ref{pb1}), as the composite $\psi$ is $0$, $\alpha=0$ and the oracle complexity would explode. In fact, \cite{allen2017katyusha} doesn't at all assume functions $f_i$ to be strongly-convex.\\ \\ Fortunately, a recent paper \cite{kovalev2019don} suggests a Katyusha version designed exactly for Problem (\ref{pb1}). This version is called L-Katyusha. Not only does it consider the non-composite case, but also suggests a modified version of Katyusha where the outer loop is removed and replaced by a new introduced randomness in the algorithm. Surprisingly, L-Katyusha has the same oracle complexity as Katyusha and hence is optimal. One more important thing is that L-Katyusha has a much simpler convergence analysis that we will present in the next section.
\begin{algorithm}\labe{katyusha}
    \SetKwInOut{Input}{Input}
    \SetKwInOut{Output}{Output}
    \Input{ $\theta_1,\theta_2\geq0$, probability $p\in(0,1]$, 
    initialize $y_0=w_0=z_0\in\mathbb{R}^d$, stepsize $\eta=\frac{\theta_2}{(1+\theta_2)\theta_1}$}
    %\Output{$\gcd(a,b)$}
    \For{$s=0,1,2,...,T$}{
            \State 
            $x_k=\theta_1 z^k+\theta_2 w^k+(1-\theta_1-\theta_2)y^k$
            
            Sample uniformly $i_{s}\in\{1,...,n\}$
            
            $v_k=\nabla f_{i_s}(x_k)-\nabla f_{i_s}(w_k)+\nabla f(w_k)$
            
            $z_{k+1}=\frac{\kappa}{\kappa+\eta}\left(\frac{\eta}{\kappa} x_k+z_k-\frac{\eta}{\beta}v^k\right)$
            
            $y_{k+1}=x_k+\theta_1(z_{k+1}-z_k)$
            
            $w_{k+1}=\left\{\begin{array}{ll}
            y_k \mathrm{\ with\ probability\ \ } p\\
            w_k \mathrm{\ with\  probability\ \ } 1-p
            \end{array}\right.$
            \EndFor}
    \caption{L-Katyusha}
\end{algorithm}


\section{Algorithm and convergence analysis}




\newpage

\begin{theo}



Let us consider our problem setting \ref{pb1}, we apply L-Katyusha with the following parameters:
Assume that parameters $\theta_1$,$\theta_2$,$\eta$ and $p$ are chosen such that the following holds

$$\gamma=\max\left\{\frac{\kappa}{\kappa+\eta},1-\theta_1(1-\theta_2),1-\frac{p\theta_1}{1+\theta_1}\right\}<1 $$

Then, 
\begin{equation}\label{eqkaty}
    \E[f(X_k)] -f(x^*)\leq \gamma^k \Phi_0
\end{equation}


where $\Phi_0=\frac{p}{\theta_2(1+\theta_1)}(f(y_0)-f(x^*))+f(w_0)-f(x^*)+\frac{\kappa(1+\frac{\eta}{\kappa})}{2\eta}\frac{p\theta_1}{\theta_2(1+\theta_1)}(f(z_0)-f(x^*))$

In particular, the L-Katyusha has a linear convergence rate.


\end{theo}


\begin{proof}
See next Subsection
\end{proof}


\begin{corollary}
Let us consider our problem setting \ref{pb1}, we apply L-Katyusha with the following parameters:
$p=\frac{1}{n}$,  $\theta_1=\min\left\{\sqrt{\frac{2n}{3\kappa}},\frac{1}{2}\right\} $, $\theta_2=\frac{1}{2}$, $\eta=\frac{\theta_2}{(1+\theta_2)\theta_1} $.

Then, the oracle complexity is:
$$ \mathcal{C}(\mathcal{A},n,\epsilon)\sim \left(n+\sqrt{\kappa n}\right)\log\left(\frac{1}{\epsilon}\right)$$
Where $\mathcal{A}$ is the L-Katyusha with the chosen parameters.
\end{corollary}

\begin{proof}
First, one should be careful about the quantity $\Phi_0$ as it does not depend only on starting points but also the parameters, namely $\kappa$. 

We structure the proof in three parts :

\textbf{\underline{Part 1}}: Let us find an upper bound for 
    $\Phi_0$
    
In order to proceed in the same manner as in corollary ?? we need to bound $\Phi_0$ by a quantity which depends only on starting points, and most importantly independent of $n$ and $\kappa$. It is hence sufficient to bound the quantities $\frac{p}{\theta_2(1+\theta_1)}$ and $\frac{\kappa(1+\frac{\eta}{\kappa})}{2\eta}\frac{p\theta_1}{\theta_2(1+\theta_1)}$  by some constants. 
In fact,
It is easy to see
$$\frac{p}{\theta_2(1+\theta_1)}=\frac{2}{n(1+\theta_1)}\leq2 $$ 
Bounding the second quantity is more subtle.

Let us distinguish the two cases :

\underline{$\theta_1=\frac{1}{2}$}


In this case $\theta_2=\frac{1}{2}$ and $\eta=\frac{1}{3}$.
Hence,
$$\frac{\kappa(1+\frac{\eta}{\kappa})}{2\eta}\frac{p\theta_1}{\theta_2(1+\theta_1)}=\frac{\kappa(1+\frac{1}{3\kappa})}{n}$$.
Since $\theta_1=\frac{1}{2}$, we have $\sqrt{\frac{2n}{3\kappa}}\geq \frac{1}{2}$, hence
$$\frac{\kappa}{n}\leq \frac{8}{3} $$
Thus,
$$\frac{\kappa(1+\frac{\eta}{\kappa})}{2\eta}\frac{p\theta_1}{\theta_2(1+\theta_1)}\leq 3$$

\underline{$\theta_1=\sqrt{\frac{2n}{3\kappa}}$}

In this case, we have $\sqrt{\frac{2n}{3\kappa}}\leq \frac{1}{2}$, $\theta=\frac{1}{2}$, $\eta=\frac{1}{\theta_1}$

$$\frac{\kappa(1+\frac{\eta}{\kappa})}{2\eta}\frac{p\theta_1}{\theta_2(1+\theta_1)}=\frac{\kappa(1+\frac{1}{3\theta_1\kappa})}{2}3\theta_1\frac{2\theta_1}{(1+\theta_1)}=3\frac{\kappa(1+\frac{1}{3\theta_1\kappa})\theta_1^2}{n(1+\theta_1)}$$
Yet,
$$3\frac{\kappa(1+\frac{1}{3\theta_1\kappa})\theta_1^2}{n(1+\theta_1)}\leq 3\frac{\kappa(1+\frac{1}{3\theta_1\kappa})\theta_1^2}{n}$$

And since,
$\frac{\kappa}{n}\theta_1^2=\frac{\kappa}{n}\frac{2n}{3\kappa}=\frac{2}{3}$, we see easily that 
$$3\frac{\kappa(1+\frac{1}{3\theta_1\kappa})\theta_1^2}{n}\leq 4$$
Therefore, the Inequality (\ref{eqkaty}) becomes $$\E[f(X_k)] -f(x^*)\leq \gamma^k \Tilde{\Phi}_0$$
Where $ \Tilde{\Phi}_0=c\left((f(y_0)-f(x^*))+(f(w_0)-f(x^*))+(f(w_0)-f(x^*)\right)$, for some constant $c$

\textbf{\underline{Part 2}}: Let us find the iteration complexity.

Let us first prove that {\huge\textbf{\textcolor{red}{TO DO}}} $$\E[f(X_k)] -f(x^*)\leq (1-\theta)^k \Phi_0$$
Where $\theta=\min\left\{\frac{1}{6\theta_1\kappa},\frac{\theta_1}{2n}\right\}$.

We have in total to distinguish between 4 cases:
\begin{enumerate}
    \item $\theta=\frac{\theta_1}{2n}$ and $\theta_1=\frac{1}{2}$
    
In this case, $\frac{1}{\theta}=4n$

    \item $\theta=\frac{\theta_1}{2n}$ and $\theta_1=\sqrt{\frac{2n}{3\kappa}}$
    
In this case, $\frac{1}{\theta}=\sqrt{6}\sqrt{\kappa n}$

    \item $\theta=\frac{1}{6\kappa \theta_1}$ and $\theta_1=\frac{1}{2}$
    
In this case, $\frac{1}{\theta}=3\kappa$,
Moreover, since $\theta_1=\frac{1}{2}$, then $\sqrt{\frac{2n}{3\kappa}}\geq \frac{1}{2}$, Hence
$\kappa \leq \frac{8}{3}n $


    \item $\theta=\frac{1}{6\kappa \theta_1}$ and $\theta_1=\sqrt{\frac{2n}{3\kappa}}$
    
    
In this case, $\frac{1}{\theta}=6\sqrt{\frac{2}{3}}\sqrt{\kappa n}$
\end{enumerate}


As we can notice, in any case, we have $\theta=\mathcal{O}\left({\frac{1}{\sqrt{n}}}\right)$, and since $n$ is large enough, we proceed to the approximation $\log(1-\theta)=-\theta$.
Now, if we want to achieve $\epsilon$-accuracy, it is enough to have $(1-\theta)^k\Tilde{\Phi}_0=\epsilon$
Dropping $\Tilde{\Phi}_0$, we get,
$k=\frac{\log(\frac{1}{\epsilon)}}{-\log(1-\theta)}$
$$k=\frac{1}{\theta}\log\left(\frac{1}{\epsilon}\right)$$

Taking into account all the 4 cases above, we conclude that $$\mathcal{I}(\mathcal{A},\epsilon)= (n+\sqrt{\kappa n})\log\left(\frac{1}{\epsilon}\right) $$

\textbf{\underline{Part 3}}: Now let us find the oracle complexity.
Consider that Algorithm \ref{katyusha} is at iteration $t$. the probability that $w_{s}=w_{s-1}$ is $p$ and in this case we don't need to compute again $\nabla f(w_k)$ assuming that it is stored in the memory, whereas $w_{s}=y_s$ with probability 1-p and we need to compute  $\nabla f(w_k)$. Recall that computing $\nabla f(w_k))$ involves $n$ oracle calls. And notice also that in every iteration we compute $\nabla f_i(x)$, hence one oracle call with probability 1.
Therefore, the expected number of oracle calls per iteration is $1+np$.
By the choice $p=\frac{1}{n}$, we conclude that the expected number of oracle calls per iteration is 2, and therefore the Oracle complexity is:

$$ \mathcal{C}(\mathcal{A},n,\epsilon)\sim \left(n+\sqrt{n\kappa} \right)\log\left(\frac{1}{\epsilon}\right)$$

\end{proof}

\section{Proof of Theorem 2.1}
The proof of Theorem 2.1 is very technical and subtantially different from the convergence analysis of SVRG. In fact, this proof relies on finding a Lyapunov function that descreases in expectation every iteration. Rather than going through how to construct such a function, we give its expression and we prove its decreasing behavior.
The Lyapunov sequence is the following : 
$$\Psi_k=\mathcal{Z}_k +\mathcal{Y}_k+\mathcal{W}_k $$
where : 
$$\mathcal{Z}_k=\frac{\beta(1+\eta\sigma)}{2\eta}\|z_k-x^*\|^2$$
The goal is to prove that $$\E[\Psi_{k+1}]\leq \gamma \E[\Psi_{k}] $$

To proof relies on the following lemmas that are proved in Supplementary materials of \cite{kovalev2019don}.
\begin{lemma}
Let $\theta_1,\theta_2>0$, $\theta_1+\theta_2\leq1$, $\eta=\frac{\theta_2}{\theta_1(1+\theta_2)}$.
We have:
\begin{equation}\label{lyapunov}
\E\left[\mathcal{Z}_{k+1}+\mathcal{Y}_{k+1}+\mathcal{W}_{k+1}\right]\leq \frac{1}{1+\eta\sigma}\E\left[\mathcal{Z}_k\right]+(1-\theta_1(1-\theta_2))\E\left[\mathcal{Y}_k\right]+\left(1-\frac{p\theta_1}{1+\theta_1}\right)\E\left[\mathcal{W}_k\right]

\end{equation}
\end{lemma}
We immediately deduce that
$$\E[\Psi_{k+1}]\leq \gamma \E[\Psi_{k}] $$
Where 
$\gamma=\max\left\{\frac{\kappa}{\kappa+\eta},1-\theta_1(1-\theta_2),1-\frac{p\theta_1}{1+\theta_1}\right\}<1$
Hence,
$$\E[\Psi_{k}]\leq \gamma^k \Psi_{0} $$
Since, $\mathcal{Z}_k \geq 0$ and $\mathcal{Y}_k \geq 0$,

we have, $$ \E\left[\mathcal{W}_k\right]\leq \E[\Psi_{k}] $$
hence,
\begin{equation}\label{eqeq}
    \E\left[f(X_k)\right]-f(x^*)\leq \gamma^k\frac{p\theta_1}{\theta_2(1+\theta_1)}\Psi_{0}
\end{equation}

Now, using the $\alpha$-strong convexity of $f$, we get
$$\|{Z}_k-x^*\|^2   \leq \frac{1}{\alpha}\left(f(x^*)-f({Z}_k)\right) $$
We conclude that, 
$$\Psi_0\leq \frac{\kappa(1+\eta\sigma)}{2\eta}\left(f({Z}_k)^-f(x^*)\right)+\frac{1}{\theta_1}(f(y_0)-f(x^*))+\frac{\theta_2(1+\theta_1)}{p\theta_1}(f(w_0)-f(x^*))$$
Plugging this into Inequality \ref{eqeq}, we get
$$\E\left[f(X_k)\right]-f(x^*)\leq \gamma^k\Phi_0 $$



\chapter{About Optimality}

Brainstorming
\begin{itemize}
    \item {Optimality is not absolute! it depends on the regime we are considering}
    \item Optimality does not mean that for every function L-Katyusha has the lowest complexity. This is completely wrong as we can easily find a counterexample...
    \item L-katyusha's Optimality means that for the class of problem \ref{pb1}, no other algorithm can has a lower oracle complexity....
    \item In fact, in general in mathematical optimization, tendency is to design algorithms on a class of problems with general enough assumptions. In contrast, as we add assumptions and have more knowledge about the structure of the function, one can come up with faster and better algorithms that eventually has an even lower oracle complexity than the ...
\end{itemize}


\chapter{Questions}
\begin{itemize}
    \item Is it better to deepen the discussion and provide more details and interpretation for the above work (such as construct examples, counterexamples...)?  or move to proximal versions Prox-SVRG/MiG/Prox-Katyusha....?
    \item Is it a good idea to suggest my own tweak of some of the previous algorithms and do experimental work? theoretical investigation might be challenging in the available time
    \item is the mathematical content so far enough
\end{itemize}



\bibliography{reference}{}
\bibliographystyle{abbrv}

\end{document}
