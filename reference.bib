@article{schmidt2017minimizing,
  title={Minimizing finite sums with the stochastic average gradient},
  author={Schmidt, Mark and Le Roux, Nicolas and Bach, Francis},
  journal={Mathematical Programming},
  volume={162},
  number={1-2},
  pages={83--112},
  year={2017},
  publisher={Springer}
}

@inproceedings{johnson2013accelerating,
  title={Accelerating stochastic gradient descent using predictive variance reduction},
  author={Johnson, Rie and Zhang, Tong},
  booktitle={Advances in neural information processing systems},
  pages={315--323},
  year={2013}
}

@misc{defazio2014finito,
    title={Finito: A Faster, Permutable Incremental Gradient Method for Big Data Problems},
    author={Aaron J. Defazio and Tib√©rio S. Caetano and Justin Domke},
    year={2014},
    eprint={1407.2710},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@inproceedings{defazio2014saga,
  title={SAGA: A fast incremental gradient method with support for non-strongly convex composite objectives},
  author={Defazio, Aaron and Bach, Francis and Lacoste-Julien, Simon},
  booktitle={Advances in neural information processing systems},
  pages={1646--1654},
  year={2014}
}


@article{hu2018dissipativity,
  title={Dissipativity theory for accelerating stochastic variance reduction: A unified analysis of SVRG and Katyusha using semidefinite programs},
  author={Hu, Bin and Wright, Stephen and Lessard, Laurent},
  journal={arXiv preprint arXiv:1806.03677},
  year={2018}
}

@article{bubeck2015convex,
  title={Convex optimization: Algorithms and complexity},
  author={Bubeck, S{\'e}bastien and others},
  journal={Foundations and Trends{\textregistered} in Machine Learning},
  volume={8},
  number={3-4},
  pages={231--357},
  year={2015},
  publisher={Now Publishers, Inc.}
}

@article{allen2017katyusha,
  title={Katyusha: The first direct acceleration of stochastic gradient methods},
  author={Allen-Zhu, Zeyuan},
  journal={The Journal of Machine Learning Research},
  volume={18},
  number={1},
  pages={8194--8244},
  year={2017},
  publisher={JMLR. org}
}