@article{schmidt2017minimizing,
  title={Minimizing finite sums with the stochastic average gradient},
  author={Schmidt, Mark and Le Roux, Nicolas and Bach, Francis},
  journal={Mathematical Programming},
  volume={162},
  number={1-2},
  pages={83--112},
  year={2017},
  publisher={Springer}
}

@inproceedings{johnson2013accelerating,
  title={Accelerating stochastic gradient descent using predictive variance reduction},
  author={Johnson, Rie and Zhang, Tong},
  booktitle={Advances in neural information processing systems},
  pages={315--323},
  year={2013}
}

@misc{defazio2014finito,
    title={Finito: A Faster, Permutable Incremental Gradient Method for Big Data Problems},
    author={Aaron J. Defazio and Tib√©rio S. Caetano and Justin Domke},
    year={2014},
    eprint={1407.2710},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@inproceedings{defazio2014saga,
  title={SAGA: A fast incremental gradient method with support for non-strongly convex composite objectives},
  author={Defazio, Aaron and Bach, Francis and Lacoste-Julien, Simon},
  booktitle={Advances in neural information processing systems},
  pages={1646--1654},
  year={2014}
}


@article{hu2018dissipativity,
  title={Dissipativity theory for accelerating stochastic variance reduction: A unified analysis of SVRG and Katyusha using semidefinite programs},
  author={Hu, Bin and Wright, Stephen and Lessard, Laurent},
  journal={arXiv preprint arXiv:1806.03677},
  year={2018}
}

@article{bubeck2015convex,
  title={Convex optimization: Algorithms and complexity},
  author={Bubeck, S{\'e}bastien and others},
  journal={Foundations and Trends{\textregistered} in Machine Learning},
  volume={8},
  number={3-4},
  pages={231--357},
  year={2015},
  publisher={Now Publishers, Inc.}
}

@article{allen2017katyusha,
  title={Katyusha: The first direct acceleration of stochastic gradient methods},
  author={Allen-Zhu, Zeyuan},
  journal={The Journal of Machine Learning Research},
  volume={18},
  number={1},
  pages={8194--8244},
  year={2017},
  publisher={JMLR. org}
}

@inproceedings{nitanda2016accelerated,
  title={Accelerated stochastic gradient descent for minimizing finite sums},
  author={Nitanda, Atsushi},
  booktitle={Artificial Intelligence and Statistics},
  pages={195--203},
  year={2016}
}

@inproceedings{lin2015universal,
  title={A universal catalyst for first-order optimization},
  author={Lin, Hongzhou and Mairal, Julien and Harchaoui, Zaid},
  booktitle={Advances in Neural Information Processing Systems},
  pages={3384--3392},
  year={2015}
}

@article{kovalev2019don,
  title={Don't Jump Through Hoops and Remove Those Loops: SVRG and Katyusha are Better Without the Outer Loop},
  author={Kovalev, Dmitry and Horvath, Samuel and Richtarik, Peter},
  journal={arXiv preprint arXiv:1901.08689},
  year={2019}
}

@inproceedings{tan2016barzilai,
  title={Barzilai-borwein step size for stochastic gradient descent},
  author={Tan, Conghui and Ma, Shiqian and Dai, Yu-Hong and Qian, Yuqiu},
  booktitle={Advances in Neural Information Processing Systems},
  pages={685--693},
  year={2016}
}

@inproceedings{tang2018rest,
  title={Rest-Katyusha: Exploiting the Solution's Structure via Scheduled Restart Schemes},
  author={Tang, Junqi and Golbabaee, Mohammad and Bach, Francis and others},
  booktitle={Advances in Neural Information Processing Systems},
  pages={427--438},
  year={2018}
}